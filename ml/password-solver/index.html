<!DOCTYPE html>
<html>
<head>
	<title>Wenyang Qian's Machine Learning</title>
	<link rel="shortcut icon" href="../../public/img/favicon-96x96.png" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Gentium+Book+Basic" rel="stylesheet">
	<link rel="stylesheet" type="text/css" href="../../public/css/homepage.css">
</head>
<body>


<h2 class="section">
    <a href="">Password Solver with Basic Genetic Algorithm.</a>
</h2>
<ul class="Aclass">
	<li class="coding-text">
	    <a>Intuitions</a><br>
        <p>We simulate the Natural Selection by picking the most "fitted" candidates for the password in each generation, and evolve them for the next generation.</p>
        <p>1. <span class="method">Fitness Function</span>: <strong class="method2">Evaluation of our candidate in the environment. </strong> 
        Here our environment is the <strong>password</strong>, and the Fitness function is used to generate a <strong>fitness score</strong> for our candidate, which decides whether it succeeds in the evolution.
        For simplicity, our password will be a string of lowercase characters.
        </p> 
        <p>2. <span class="method">Initial Population</span>: 
        In this basic application, we assume the initial population is entirely random, by creating a set of strings with random lowercase characters.
        </p>
        
        <p>3. <span class="method">Breeder(Natural) Selection</span>: <strong class="method2">The Core Process of Darwin's evolution.</strong> 
        We only select/allow the top candidates (base on their fitness scores) to reproduce offsprings, and also pick a small collection of strings randomly to add some environment uncertainty.
        </p>
        
        <p>4. <span class="method">Genetic Crossover</span>: <strong class="method2">When producing offsprings, genes from parents are combined. </strong> This process is called <strong>Chromosomal crossover</strong> in biology.
        Here, each character in the offsprings has the same chance of coming from either parent.
        </p>
            
        <p>5. <span class="method">Mutations</span>: <strong class="method2">Fraction of genes are shifted at random.</strong> Here, we pick some percentage of the offspring candidates at random and mutate only one of their string characters.
        </p>
        
        <p>
        <h3>Basic Tuning Parameters and default values:</h3>
        <strong>Password Length</strong> = 100<br>
        <strong>Initial Population Size</strong> = 10000<br>
        <strong>Top Breeder Selection</strong> = 22%<br>
        <strong>Lucky Breeder Selection</strong> = 3%<br>
        <strong>Children Per Couple</strong> = 8<br>
        <strong>Mutation Rate</strong> = 1%<br>
        </p>

    </li>
    
    <li class="coding-text">
	    <a>Results</a><br>
	    <p>With default parameters, we found the password in the <strong>24th</strong> generation. When the population converges, the median fitness score is around <strong>95.0</strong>. The following plot shows the distributions of the fitness of the candidates at various generations.</p>
        <img alt="default" style="display: block; margin: 10px auto; width:55%; min-width: 500px;" src="img/default.png">

        <p>By using a wider Breeder Selection (using 50% from Top Breeder) and ignore Lucky Breeder (as it turns out insignificant), we found the password in the <strong>34th</strong> generation. When the population converges, the median fitness score is around <strong>94.0</strong>. 
        It is apparent that this convergence is much slower in comparison.</p>
        <img alt="larger_selection" style="display: block; margin: 10px auto; width:55%; min-width: 500px;" src="img/larger_selection.png">
    
        <p>In contrast, using random selection instead of Breeder Selection, we could not find the password even at 80th generation. No improvement over generations.</p>
        <img alt="random_selection" style="display: block; margin: 10px auto; width:55%; min-width: 500px;" src="img/random_selection.png">
        
        <p><span class="method">Observations</span>: In plots not shown here, we also observed that Mutation Rate and Lucky Breeder Selection do not have much influence on the convergences and distributions, for this particular application.</p>
    </li>

    <li class="coding-text">
	    <a>Python Implementation</a><br>

<pre class="prettyprint lang-python3 ">
#1. Fitness Function:
def fitness(password, test_word):

    if (len(test_word) != len(password)):
        print("Length incompatible!")
        return
    else:
        score = sum([password[i] == test_word[i] for i in range(len(password))])
        return score*100/len(password)

</pre>
<pre class="prettyprint lang-python3 ">
#2. Initial Population:
def getRandomChar():
    return random.choice(string.ascii_lowercase)

def generateAWord(length):
    return "".join(getRandomChar() for _ in range(length))

def generateFirstPopulation(length, popsize):
    return [generateAWord(length) for _ in range(popsize)]

</pre>
<pre class="prettyprint lang-python3 ">
#3. Natural/Breeder Selection:
def getPopulationSortedByFitness(population, password):
    return sorted(population, key=lambda x: fitness(x, password), reverse=True)
    
def selectFromSortedPopulation(sorted_population, best_pop, lucky_pop):
    nextPopulation = []
    for i in range(best_pop):
        nextPopulation.append(sorted_population[i])
    for i in range(lucky_pop):
        nextPopulation.append(random.choice(sorted_population))
    random.shuffle(nextPopulation)
    return nextPopulation
    
</pre>
<pre class="prettyprint lang-python3 ">   
#4. Breeding/Genetic Crossover:
def createAChild(parent1, parent2):
    child = ""
    for i in range(len(parent1)):
        if random.random() < 0.5:
            child += parent1[i]
        else:
            child += parent2[i]
    return child

def createChildren(breeders, child_number):
    children = []
    for i in range(1, len(breeders), 2):
        children += [createAChild(breeders[i], breeders[i-1]) for _ in range(child_number)]
    random.shuffle(children)
    return children

</pre>

<pre class="prettyprint lang-python3 ">   
#5. Mutation:
def mutateWord(word):
    index = int(random.random()*len(word))
    return word[:index] + getRandomChar() + word[index+1:]

def mutatePopulation(population, mutation_rate=0.1):
    for i in range(len(population)):
        if random.random() <= mutation_rate:
            population[i] = mutateWord(population[i])
    return population
    
</pre>

<pre class="prettyprint lang-python3 ">
#Main Loop:
def main():
    PASSWORD = generateAWord(100)
    FIRST_POP_SIZE = 10000
    BEST_POP_SIZE = 0.22
    LUCKY_POP_SIZE = 0.03
    CHILDREN_NUMBER = 8
    MUTATION_RATE = 0.01
    TOTAL_GEN = 40
    
    PLOT_SLICES = 5
    plt.figure(figsize=(15,10))

    found = False
    iteration = 1
    
    while not found:
        
        if iteration == 1:
            population = generateFirstPopulation(len(PASSWORD), FIRST_POP_SIZE)

        print("Iteration: ", iteration)
        print("Population: ", len(population))
        population = getPopulationSortedByFitness(population, PASSWORD)
        breeders = selectFromSortedPopulation(population, int(BEST_POP_SIZE*len(population)), int(LUCKY_POP_SIZE*len(population)))
        children = createChildren(breeders, CHILDREN_NUMBER)    
        nextPopulation = mutatePopulation(children, MUTATION_RATE)
        
        if iteration % (TOTAL_GEN//PLOT_SLICES) == 1: getHistogram(PASSWORD, nextPopulation, iteration)
        if foundPassword(nextPopulation, PASSWORD):
            print("PASSWORD FOUND!")
            if iteration > TOTAL_GEN: found = True
            print(nextPopulation.count(PASSWORD)/len(nextPopulation)*100, "%")
            print("median: ", fitness(PASSWORD, sorted(nextPopulation, key=lambda x: fitness(x, PASSWORD))[len(nextPopulation)//2]))
        else:
            population = nextPopulation
        
        iteration += 1
        if len(population) == 0: 
            print("Population died...")
            break

#Helper Functions:
def foundPassword(population, password):
    for pop in population:
        if pop == password:
            return True
    return False
    
def getCounts(password, population):
    return [fitness(password, x) for x in population]
    
def getHistogram(password, population, which_generation):
    counts = getCounts(password, population)
    plt.hist(counts, weights=np.ones(len(counts)) / len(counts), label="Gen {}".format(which_generation))
    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))
    plt.legend()

</pre>

    </li>
</ul>

<div class="footer">
    <a title="Back to Previous" href="https://wyqian1027.github.io/public/machine-learning.html"><i class="fas fa-igloo"></i></a>
	<em>Last updated February 27, 2019 by Wenyang Qian.</em>
</div>

</body>
</html>