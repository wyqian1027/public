<!DOCTYPE html>
<html>
<head>
	<title>Shape Recognition: Machine Learning application</title>
	<link rel="stylesheet" type="text/css" href="ml.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
	<link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Sans+Pro:400" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Indie+Flower" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css?family=Space+Mono" rel="stylesheet">
</head>
<body>
<div class="container">
<!--*******************************************-->
<h1 id="title">Shape Recognition: a Machine Learning Demo </h1> 
<p>All codes are in Python.</p>
<!--<img src="https://cdn-images-1.medium.com/max/1200/1*3CXBOKNql4qS-lRyHT3pqw.png" width="300px" align="right" >-->
<br/><br/><br/>
<h1> <i class="far fa-lightbulb"></i> What is Machine Learning?</h1>
<p>Machine learning is a field in computer science that uses statistical techniques to enables computer systems to <spam class="highlight"> "learn" with data </spam>, without being explicitly programmed.</p>
<p>Machine learning is usually advantageous in complex systems where traditional algorithmic methods cannot be easily or even possibly defined.</p>
<p>Data is usually separated into <span class="highlight"> Training set</span> where inputs and outcome (or labels, response, expectations) are known beforehand and <span class="highlight"> Test set</span> where the computers are expected to predict the outcome based on the input.</p>




<!--*******************************************-->
<br/>
<h1> <i class="fas fa-search-plus"></i> Where do we use Machine Learning?</h1>
<ul>
    <li>Spam Detection</li>
    <li>Audio/Visual Recognition <span class="highlight" style="color:red;">--> Shape Recognition</span></li>
    <li>Web Search Engine</li>
    <li>Autonomous Driving</li>
    <li>Fundamental Research in Science</li>
    <li>(more)</li>
    
</ul>

<!--*******************************************-->
<br/>
<h1> <i class="fas fa-shapes"></i> Data Acquisition:</h1>
<p> Collecting <span class="highlight">useful and clean data</span> proves to be a challenging task. For the current project, we have asked <span class="highlight_num">10</span> volunteers to draw a total of <span class="highlight_num">640</span> shapes.</p>
<p> Every participant needs to draw <span class="highlight_num">8</span> shapes for each of the following <span class="highlight_num">eight</span> categories: <span style="font-weight: 700"><i class="far fa-circle"></i> (Circle), Triangle, <i class="far fa-square"></i> (Square), Pentagon, Rhombus, <i class="far fa-star"></i> (Star), Trefoil, <i class="far fa-heart"></i> (Heart)</span></p>
<p> All Images are taken by iPhone (initial total size = <span class="highlight_num">543.4 MB</span>) and then processed into grayscale, dimensionally-reduced numpy packages. (final size = <span class="highlight_num">8.2 MB</span>)</p>
<ul>
    <li>
        <p>Selected unprocessed raw images:</p>
        <img src="raw.png">
    </li>
    <li>
        <p> Grayscaled images using Python Imaging Library (PIL): <span class="highlight" style="font-size:0.9em; font-weight:100;">--> for the moment, color is not a feature!</span></p>
        <img src="gs2448.png">
    </li>
    <li>
        <p> Dimensionality reduction of images from <span class="highlight_num">2448 X 2448 </span> = 5992704 pixels to <span class="highlight_num">40 X 40 = 1600 </span>pixels:</p>
        <img src="gs40.png">
    </li>
    <li>
        <p> Flattening these images as two numpy arrays/packages, <span class="highlight">data </span> and <span class="highlight">label </span>: </p>
        <a href="datasets/data.npy"><i class="far fa-file-alt"></i> <span class="download"> data.npy </span>  </a>
        <a href="datasets/label.npy"><i class="far fa-file-alt"></i> <span class="download"> label.npy </span>  </a>
        <a href="label_dict.png"><i class="far fa-file-alt"></i> <span class="download"> label dictionary </span> </a>
        <p> Dimensions: <span class="code"> data.shape = (640, 1600), label.shape = (640,) </span> </p>
        <p> PS: Be free to download and use the datasets. I also developed additional datasets with much-reduced dimensions of <span class="highlight_num">25 X 25 = 625 </span> : <a href="datasets/data_DIM625.npy"><i class="far fa-file-alt"></i> <span class="download" style="font-size:1em;" > data_DIM625.npy </span>  </a><a href="datasets/label_DIM625.npy"><i class="far fa-file-alt"></i> <span class="download" style="font-size:1em;"> label_DIM625.npy </span></a></p>
    </li>
</ul>

<!--*******************************************-->
<br/>
<h1> <i class="fas fa-columns"></i> Train Test Split:</h1>
<p> We need to divide all the data and label into training set and test set. Then, machine can be trained by various methods on the training set to make predictions on the test set.</p>
<p> If the computer is trained with the knowledge of labels, then it is <span class="highlight">supervised learning</span>. Otherwise, it is called <span class="highlight">unsupervised learning</span>. For this project, most methods I show here will be in the former category.</p>
<p> Test size of <span class="highlight_num">0.25</span> is used --> test set = 160, training set = 480. Varied random seed is used to produce different masks on the test set and training set. For example,</p>
<p> <span class="code"> trainx, trainy, testx, testy = train_test_split(data, labels, split_size=0.25, random_seed=1, split_window=0)</span></p>
<p> <strong>4-fold Cross Validations</strong> are done for each method introduced below. Same random seed = 2 is used.</p>

<!--*******************************************-->
<br/>
<h1> <i class="fas fa-wrench"></i> Method 1: K-Nearest Neighbors (kNN)</h1>
<p>kNN is probably the simpliest machine learning method. </p>
<p>One need first define a proper distance function to compute the relative closeness between two image arrays. Then for each test data, scan all training data to find the closest image array and return its corresponding label. For example, </p>
<img class="code_img" src="method_file/knn-1.png">
<p>Final Result, averaged error rate ~ <span class="highlight_num">6.56 %</span>, not bad at all!</p>
<img class="code_img" src="method_file/knn-result.png">
<p>Example of a misclassified pentagon: test image #118, and its nearest neighbor training image #457</p>
<img class="case_img" src="method_file/knn-case.png">
<img class="case_img" src="method_file/knn-case2.png">
<p>One can also directly use kNN from library, such as <span class="code"> sklearn.neighbors.BallTree, sklearn.neighbors.KDTree</span> and get the same result.</p>


<!--*******************************************-->
<br/>
<h1> <i class="fas fa-wrench"></i> Method 2: Gaussian Generative Model (GGM)</h1>
<p> Generative model uses statistical techniques to find the highest probable label for the test data.</p>
<p> Here, by learning the training data and training label, we generate a statistical multivariate Gaussian distribution for each pattern. Then, for each test data, we compute the probablity for each label according to the distribution, and pick the highest one. This is direct application of <span class="highlight">Bayes' Theorem</span>:</p>
<p> $$ \text{Probablity}\Big(\text{label}=j\Big|x\Big) = \frac{\text{Probablity}(\text{label}=j)*\text{Probablity}\Big(x\Big|\text{label}=j\Big)}{\text{Probablity}(x)} \propto \text{Probablity}\Big(x\Big|\text{label}=j\Big),$$</p>
<p> where \(\text{Probablity}\Big(x\Big|\text{label}=j\Big)\) is the value of Gaussian distribution for label j of data x.</p>
<p> One can either use multivariate Gaussian from library <span class="code"> from scipy.stats import multivariate_normal </span> or define your own:</p>
<img class="code_img_lg" src="method_file/GGM-1.png">
<p> Final Result, averaged error rate ~ <span class="highlight_num">1.25 %</span>, <span class="highlight"> significant improvement!</span></p>
<img class="code_img" src="method_file/GGM-result.png">

<!--*******************************************-->
<br/>
<h1> <i class="fas fa-wrench"></i> Method 3: Multi-class Perceptron (MP)</h1>
<p> <span class="highlight">Background </span>: For a binary classification problem, one can use Perceptron method to predict the test labels. Perceptron uses basic linear algebra. It predicts the label by evaluating the sign of a linear estimator function. For example, </p>
<p> $$ y = W \cdot \vec{x} + b, \text{ where $W$ is the weight matrix, $\vec{x}$ is an instance from the data, $b$ is the bias.} $$</p>
<p> The loss function is then (\(y\) - label) for each data x. We can find the optimal \(W\) and \(b\), by using gradient descent method.</p>
<p> What we have is a multi-class (8 classes) classification problem, so we extend the Perceptron algorithm to find and return the label with the highest score. One possible estimator can be:</p>
<img class="code_img_sm" src="method_file/perc-1.png">
<p> Then we can train the training set using gradient descent. <span class="highlight">One important note</span>: For more complicated data, one might need to use Stochastic gradient descent or mini-batch Stochastic gradient descent.</p>
<img class="code_img" src="method_file/perc-2.png">
<p> Keeping track of dimensions is a key: <span class="code"> w.shape = (8, 1600), b.shape = (8)</p>
<p> Final Result, averaged error rate ~ <span class="highlight_num">8.75 %</span>, convergences of training set is <span class="highlight_num">9 - 12</span> iterations, surprisingly fast! </p>
<img class="code_img" src="method_file/perc-result.png">



</div>
<script type="text/javascript" src="ml.js"></script>


</body>
</html>